{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the RAW dataset to TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from pascal_voc_writer import Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('database', 'Pedestrian',\n",
    "                           'Training data directory')\n",
    "tf.app.flags.DEFINE_string('image_dir', 'images',\n",
    "                           'Training data directory')\n",
    "tf.app.flags.DEFINE_string('test_dir', 'test_images',\n",
    "                           'Validation data directory')\n",
    "tf.app.flags.DEFINE_string('csv_file', 'TownCentre-groundtruth.top',\n",
    "                           'Path to the csv annotation file')\n",
    "tf.app.flags.DEFINE_string('annotations_dir', 'Annotations',\n",
    "                           '(Relative) path to annotations directory.')\n",
    "tf.app.flags.DEFINE_string('xml_dir', 'Annotations/xmls',\n",
    "                           'Validation data directory')\n",
    "tf.app.flags.DEFINE_string('output_dir', '', 'Path to directory to output TFRecords.')\n",
    "tf.app.flags.DEFINE_string('label_map_path', 'Annotations/pedestrian_label_map.pbtxt',\n",
    "                           'Path to label map proto')\n",
    "tf.app.flags.DEFINE_boolean('ignore_difficult_instances', False, 'Whether to ignore '\n",
    "                            'difficult instances')\n",
    "tf.app.flags.DEFINE_integer('num_shards', 10, 'Number of TFRecord shards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert *.csv to *.xml(s)\n",
    "\n",
    "The default csv annotation format is: `<filename>;<xmin;ymin;xmax;ymax>;<class_id>`\n",
    "\n",
    "For example, this is how we will process csv file(s) and convert it into xml(s)\n",
    "\n",
    "```python\n",
    "obj_attributes_string = '00000.ppm;774;411;815;446;11'\n",
    "\n",
    "obj_attributes_split = obj_attributes_string.split(';')\n",
    "\n",
    "writer = Writer(obj_attributes_split[0], 300, 300, database='Unknown')\n",
    "writer.addObject(\n",
    "    obj_attributes_split[-1],\n",
    "    obj_attributes_split[1],\n",
    "    obj_attributes_split[2],\n",
    "    obj_attributes_split[3],\n",
    "    obj_attributes_split[4])\n",
    "writer.save('out.xml')\n",
    "```\n",
    "\n",
    "However, in this dataset, the annotation format will be:\n",
    "\n",
    "```\n",
    "personNumber, frameNumber, headValid, bodyValid, headLeft, headTop, headRight, headBottom, bodyLeft, bodyTop, bodyRight, bodyBottom\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To make it easier to access the annotation points, this dictionary will be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_dict = {\n",
    "    'personNumber': 0,\n",
    "    'frameNumber': 1,\n",
    "    'headValid': 2,\n",
    "    'bodyValid': 3,\n",
    "    'headLeft': 4,\n",
    "    'headTop': 5,\n",
    "    'headRight': 6,\n",
    "    'headBottom': 7,\n",
    "    'bodyLeft': 8,\n",
    "    'bodyTop': 9,\n",
    "    'bodyRight': 10,\n",
    "    'bodyBottom': 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3299: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def create_xml_annotation(database, csv_file_path, image_dir, xml_dir, factor = 2):\n",
    "    if not os.path.isdir(xml_dir):\n",
    "            os.makedirs(xml_dir)\n",
    "\n",
    "    data = pd.read_csv('TownCentre-groundtruth.top', header=None)\n",
    "    train_size = 4501\n",
    "\n",
    "    for frame_number in range(train_size):\n",
    "        Frame = data.loc[data[1] == frame_number] \n",
    "        x1 = list(Frame[8])\n",
    "        y1 = list(Frame[11])\n",
    "        x2 = list(Frame[10])\n",
    "        y2 = list(Frame[9])\n",
    "        points = [[(round(x1_), round(y1_)), (round(x2_), round(y2_))] for x1_,y1_,x2_,y2_ in zip(x1,y1,x2,y2)]\n",
    "        \n",
    "        image_path = os.path.join(image_dir, f'{frame_number}.jpg')\n",
    "        image = Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        writer = Writer(image_path, width, height, database=FLAGS.database)\n",
    "\n",
    "        for point in points:\n",
    "\n",
    "            top_left = point[0]\n",
    "            bottom_right = point[1]\n",
    "\n",
    "            if top_left[0] > bottom_right[0]:\n",
    "                xmax, xmin = top_left[0] // factor, bottom_right[0] // factor\n",
    "            else:\n",
    "                xmin, xmax = top_left[0] // factor, bottom_right[0] // factor\n",
    "\n",
    "            if top_left[1] > bottom_right[1]:\n",
    "                ymax, ymin = top_left[1] // factor, bottom_right[1] // factor\n",
    "            else:\n",
    "                ymin, ymax = top_left[1] // factor, bottom_right[1] // factor\n",
    "            \n",
    "            # Validate image dimensions\n",
    "            if xmin > width:\n",
    "                xmin = width\n",
    "            if xmax > width:\n",
    "                xmax = width\n",
    "            if ymin > height:\n",
    "                ymin = height\n",
    "            if ymax > height:\n",
    "                ymax = height\n",
    "\n",
    "            writer.addObject('pedestrian', xmin, ymin, xmax, ymax)\n",
    "        \n",
    "        xml_name = f'{frame_number}.xml'\n",
    "        writer.save(os.path.join(xml_dir, xml_name))\n",
    "\n",
    "        \n",
    "def main(unused_argv):\n",
    "    create_xml_annotation(FLAGS.database, FLAGS.csv_file, FLAGS.image_dir, FLAGS.xml_dir)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the PASCAL VOC data to TFRecordÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import contextlib2\n",
    "from lxml import etree\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.dataset_tools import tf_record_creation_util\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "\n",
    "DATASETS = ['GTSDB_VOC', 'Pedestrian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_tf_example(data,\n",
    "                       label_map_dict,\n",
    "                       ignore_difficult_instances=False):\n",
    "    \"\"\"Convert XML derived dict to tf.Example proto.\n",
    "    \n",
    "    Notice that this function normalizes the bounding box coordinates provided\n",
    "    by the raw data.\n",
    "    \n",
    "    Args:\n",
    "        data: dict holding PASCAL XML fields for a single image (obtained by\n",
    "            running dataset_util.recursive_parse_xml_to_dict)\n",
    "        dataset_directory: Path to root directory holding PASCAL dataset\n",
    "        label_map_dict: A map from string label names to integers ids.\n",
    "        image_subdirectory: String specifying subdirectory within the\n",
    "            PASCAL dataset directory holding the actual image data.\n",
    "        ignore_difficult_instances: Whether to skip difficult instances in the\n",
    "            dataset  (default: False).\n",
    "    \n",
    "    Returns:\n",
    "        example: The converted tf.Example.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: if the image pointed to by data['filename'] is not a valid JPEG\n",
    "    \"\"\"\n",
    "    img_path = os.path.join(data['folder'], data['filename'])\n",
    "    with tf.gfile.GFile(img_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    if image.format != 'JPEG':\n",
    "        raise ValueError('Image format is not JPEG')\n",
    "    key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "\n",
    "    width = int(data['size']['width'])\n",
    "    height = int(data['size']['height'])\n",
    "\n",
    "    xmin = []\n",
    "    ymin = []\n",
    "    xmax = []\n",
    "    ymax = []\n",
    "    classes = []\n",
    "    classes_text = []\n",
    "    truncated = []\n",
    "    poses = []\n",
    "    difficult_obj = []\n",
    "    if 'object' in data:\n",
    "        for obj in data['object']:\n",
    "            difficult = bool(int(obj['difficult']))\n",
    "            if ignore_difficult_instances and difficult:\n",
    "                continue\n",
    "\n",
    "            difficult_obj.append(int(difficult))\n",
    "\n",
    "            xmin.append(float(obj['bndbox']['xmin']) / width)\n",
    "            ymin.append(float(obj['bndbox']['ymin']) / height)\n",
    "            xmax.append(float(obj['bndbox']['xmax']) / width)\n",
    "            ymax.append(float(obj['bndbox']['ymax']) / height)\n",
    "            classes_text.append(obj['name'].encode('utf8'))\n",
    "            classes.append(label_map_dict[obj['name']])\n",
    "            truncated.append(int(obj['truncated']))\n",
    "            poses.append(obj['pose'].encode('utf8'))\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(\n",
    "            data['filename'].encode('utf8')),\n",
    "        'image/source_id': dataset_util.bytes_feature(\n",
    "            data['filename'].encode('utf8')),\n",
    "        'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "        'image/object/difficult': dataset_util.int64_list_feature(difficult_obj),\n",
    "        'image/object/truncated': dataset_util.int64_list_feature(truncated),\n",
    "        'image/object/view': dataset_util.bytes_list_feature(poses),\n",
    "        }))\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_record(output_filename,\n",
    "                     num_shards,\n",
    "                     label_map_dict,\n",
    "                     annotations_dir,\n",
    "                     examples):\n",
    "    \"\"\"Creates a TFRecord file from examples.\n",
    "    Args:\n",
    "      output_filename: Path to where output file is saved.\n",
    "      num_shards: Number of shards for output file.\n",
    "      label_map_dict: The label map dictionary.\n",
    "      annotations_dir: Directory where annotation files are stored.\n",
    "      examples: Examples to parse and save to tf record.\n",
    "    \"\"\"\n",
    "    with contextlib2.ExitStack() as tf_record_close_stack:\n",
    "        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n",
    "            tf_record_close_stack, output_filename, num_shards)\n",
    "        for idx, example in enumerate(examples):\n",
    "            if idx % 100 == 0:\n",
    "                logging.info(f'On image {idx} of {len(examples)}')\n",
    "            xml_path = os.path.join(annotations_dir, 'xmls', example + '.xml')\n",
    "            \n",
    "            if not os.path.exists(xml_path):\n",
    "                logging.warning(f'Could not find {xml_path}, ignoring example.')\n",
    "                continue\n",
    "            with tf.gfile.GFile(xml_path, 'r') as fid:\n",
    "                xml_str = fid.read()\n",
    "            xml = etree.fromstring(xml_str)\n",
    "            data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n",
    "\n",
    "            try:\n",
    "                tf_example = dict_to_tf_example(\n",
    "                    data,\n",
    "                    label_map_dict)\n",
    "                if tf_example:\n",
    "                    shard_idx = idx % num_shards\n",
    "                    output_tfrecords[shard_idx].write(\n",
    "                        tf_example.SerializeToString())\n",
    "            except ValueError:\n",
    "                logging.warning(f'Invalid example: {xml_path}, ignoring.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoangtnm/workspace/models/research/object_detection/utils/dataset_util.py:75: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if not xml:\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def main(unused_argv):\n",
    "    if FLAGS.database not in DATASETS:\n",
    "        raise ValueError(f'set must be in : {DATASETS}')\n",
    "    \n",
    "    data_dir = os.getcwd()\n",
    "    label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n",
    "    \n",
    "    logging.info(f'Reading from {FLAGS.database} dataset.')\n",
    "    annotations_dir = os.path.join(data_dir, 'Annotations')\n",
    "    examples_path = os.path.join(annotations_dir, 'trainval.txt')\n",
    "    examples_list = dataset_util.read_examples_list(examples_path)\n",
    "    \n",
    "    # Test images are not included in the downloaded data set, so we shall perform\n",
    "    # our own split.\n",
    "    random.seed(42)\n",
    "    random.shuffle(examples_list)\n",
    "    num_examples = len(examples_list)\n",
    "    num_train = int(0.7 * num_examples)\n",
    "    train_examples = examples_list[:num_train]\n",
    "    val_examples = examples_list[num_train:]\n",
    "    logging.info('%d training and %d validation examples.',\n",
    "        len(train_examples), len(val_examples))\n",
    "    \n",
    "    train_output_path = os.path.join(FLAGS.output_dir, 'train.record')\n",
    "    val_output_path = os.path.join(FLAGS.output_dir, 'val.record')\n",
    "    \n",
    "    create_tf_record(\n",
    "        train_output_path,\n",
    "        FLAGS.num_shards,\n",
    "        label_map_dict,\n",
    "        annotations_dir,\n",
    "        train_examples)\n",
    "    create_tf_record(\n",
    "        val_output_path,\n",
    "        FLAGS.num_shards,\n",
    "        label_map_dict,\n",
    "        annotations_dir,\n",
    "        val_examples)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
