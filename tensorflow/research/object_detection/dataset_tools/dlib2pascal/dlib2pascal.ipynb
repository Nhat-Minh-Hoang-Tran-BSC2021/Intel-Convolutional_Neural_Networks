{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covert imglab format to PASCAL VOC format\n",
    "\n",
    "This notebook is designed to give insights how imglab formart used in `dlib` library can be converted to PASCAL VOC format in order to use TensorFlow Object Detection API\n",
    "\n",
    "Data directory structure:\n",
    "\n",
    "```\n",
    ".\n",
    "|--Annotations\n",
    "| |\n",
    "| -xmls\n",
    "|--dataset1\n",
    "|--dataset2\n",
    "|--dataset1.xml\n",
    "|--dataset2.xml\n",
    "|--merged.xml\n",
    "|--train.record-?????-of-00010\n",
    "|--val.record-?????-of-00010\n",
    "```\n",
    "\n",
    "The `Annotations` folder will be the place where PASCAL VOC annotations will be written to.\n",
    "\n",
    "The `train.record` and `val.record` are TFRecord files which can be used for the TensorFlow Object Detection API to train models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "from PIL import Image\n",
    "from pascal_voc_writer import Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('database', 'AutoLens',\n",
    "                           'Database name')\n",
    "tf.app.flags.DEFINE_string('imglab_xml', 'merged.xml',\n",
    "                           'Path to the imglab annotation file')\n",
    "tf.app.flags.DEFINE_string('xml_dir', 'Annotations/xmls',\n",
    "                           'Path to PASCAL VOC xml files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing XML\n",
    "\n",
    "```xml\n",
    "<name>imglab dataset</name>\n",
    "<comment>Created by imglab tool.</comment>\n",
    "<images>\n",
    "  <image file='Images/1Img_1.jpg'>\n",
    "    <box top='665' left='1366' width='187' height='141'>\n",
    "      <label>label</label>\n",
    "    </box>\n",
    "    <box top='607' left='1113' width='54' height='70'>\n",
    "      <label>label</label>\n",
    "    </box>\n",
    "    <box top='389' left='738' width='337' height='282' ignore='1'>\n",
    "      <label>label</label>\n",
    "    </box>\n",
    "    <box top='413' left='1171' width='707' height='230' ignore='1'>\n",
    "      <label>label</label>\n",
    "    </box>\n",
    "  </image>\n",
    "</images>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xml_annotation(database, imglab_file_path, xml_dir):\n",
    "    if not os.path.isdir(xml_dir):\n",
    "        os.makedirs(xml_dir)\n",
    "    \n",
    "    tree = et.parse('merged.xml')  \n",
    "    root = tree.getroot()\n",
    "    images = root.find('images')\n",
    "    \n",
    "    # Finds the first child with the 'images' tag\n",
    "    for image in images.iter(tag='image'):\n",
    "        image_path = image.attrib['file']\n",
    "        folder = os.path.dirname(image_path)\n",
    "        raw_filename = os.path.basename(image_path)\n",
    "        filename, file_extension = os.path.splitext(raw_filename)\n",
    "\n",
    "        img = Image.open(image_path)\n",
    "        width, height = img.size\n",
    "\n",
    "        writer = Writer(image_path, width, height, database=database)\n",
    "\n",
    "        num_objects = 0\n",
    "        for box in image:\n",
    "            if 'ignore' in box.attrib.keys():\n",
    "                continue\n",
    "            num_objects += 1\n",
    "            xmin = box.attrib['left']\n",
    "            ymin = box.attrib['top']\n",
    "            xmax = int(xmin) + int(box.attrib['width'])\n",
    "            ymax = int(ymin) + int(box.attrib['height'])\n",
    "            writer.addObject(\n",
    "                name=box[0].text,\n",
    "                xmin=xmin,\n",
    "                ymin=ymin,\n",
    "                xmax=xmax,\n",
    "                ymax=ymax\n",
    "            )\n",
    "        if num_objects > 0:\n",
    "            writer.save(os.path.join(FLAGS.xml_dir, f'{folder}_{filename}.xml'))\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "    create_xml_annotation(FLAGS.database, FLAGS.imglab_xml, FLAGS.xml_dir)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the PASCAL VOC data to TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import contextlib2\n",
    "from lxml import etree\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.dataset_tools import tf_record_creation_util\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('annotations_dir', 'Annotations',\n",
    "                           '(Relative) path to annotations directory.')\n",
    "tf.app.flags.DEFINE_string('output_dir', '', 'Path to directory to output TFRecords.')\n",
    "tf.app.flags.DEFINE_string('label_map_path', 'autolens_label_map.pbtxt',\n",
    "                           'Path to label map proto')\n",
    "tf.app.flags.DEFINE_boolean('ignore_difficult_instances', False, 'Whether to ignore '\n",
    "                            'difficult instances')\n",
    "tf.app.flags.DEFINE_integer('num_shards', 10, 'Number of TFRecord shards')\n",
    "\n",
    "DATASETS = ['GTSDB_VOC', 'AutoLens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_tf_example(data,\n",
    "                       label_map_dict,\n",
    "                       ignore_difficult_instances=False):\n",
    "    \"\"\"Convert XML derived dict to tf.Example proto.\n",
    "    \n",
    "    Notice that this function normalizes the bounding box coordinates provided\n",
    "    by the raw data.\n",
    "    \n",
    "    Args:\n",
    "        data: dict holding PASCAL XML fields for a single image (obtained by\n",
    "            running dataset_util.recursive_parse_xml_to_dict)\n",
    "        dataset_directory: Path to root directory holding PASCAL dataset\n",
    "        label_map_dict: A map from string label names to integers ids.\n",
    "        image_subdirectory: String specifying subdirectory within the\n",
    "            PASCAL dataset directory holding the actual image data.\n",
    "        ignore_difficult_instances: Whether to skip difficult instances in the\n",
    "            dataset  (default: False).\n",
    "    \n",
    "    Returns:\n",
    "        example: The converted tf.Example.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: if the image pointed to by data['filename'] is not a valid JPEG\n",
    "    \"\"\"\n",
    "    img_path = os.path.join(data['folder'], data['filename'])\n",
    "    with tf.gfile.GFile(img_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    if image.format != 'JPEG':\n",
    "        raise ValueError('Image format is not JPEG')\n",
    "    key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "\n",
    "    width = int(data['size']['width'])\n",
    "    height = int(data['size']['height'])\n",
    "\n",
    "    xmin = []\n",
    "    ymin = []\n",
    "    xmax = []\n",
    "    ymax = []\n",
    "    classes = []\n",
    "    classes_text = []\n",
    "    truncated = []\n",
    "    poses = []\n",
    "    difficult_obj = []\n",
    "    if 'object' in data:\n",
    "        for obj in data['object']:\n",
    "            difficult = bool(int(obj['difficult']))\n",
    "            if ignore_difficult_instances and difficult:\n",
    "                continue\n",
    "\n",
    "            difficult_obj.append(int(difficult))\n",
    "\n",
    "            xmin.append(float(obj['bndbox']['xmin']) / width)\n",
    "            ymin.append(float(obj['bndbox']['ymin']) / height)\n",
    "            xmax.append(float(obj['bndbox']['xmax']) / width)\n",
    "            ymax.append(float(obj['bndbox']['ymax']) / height)\n",
    "            classes_text.append(obj['name'].encode('utf8'))\n",
    "            classes.append(label_map_dict[obj['name']])\n",
    "            truncated.append(int(obj['truncated']))\n",
    "            poses.append(obj['pose'].encode('utf8'))\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(\n",
    "            data['filename'].encode('utf8')),\n",
    "        'image/source_id': dataset_util.bytes_feature(\n",
    "            data['filename'].encode('utf8')),\n",
    "        'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "        'image/object/difficult': dataset_util.int64_list_feature(difficult_obj),\n",
    "        'image/object/truncated': dataset_util.int64_list_feature(truncated),\n",
    "        'image/object/view': dataset_util.bytes_list_feature(poses),\n",
    "        }))\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_record(output_filename,\n",
    "                     num_shards,\n",
    "                     label_map_dict,\n",
    "                     annotations_dir,\n",
    "                     examples):\n",
    "    \"\"\"Creates a TFRecord file from examples.\n",
    "    Args:\n",
    "      output_filename: Path to where output file is saved.\n",
    "      num_shards: Number of shards for output file.\n",
    "      label_map_dict: The label map dictionary.\n",
    "      annotations_dir: Directory where annotation files are stored.\n",
    "      examples: Examples to parse and save to tf record.\n",
    "    \"\"\"\n",
    "    with contextlib2.ExitStack() as tf_record_close_stack:\n",
    "        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n",
    "            tf_record_close_stack, output_filename, num_shards)\n",
    "        for idx, example in enumerate(examples):\n",
    "            if idx % 100 == 0:\n",
    "                logging.info(f'On image {idx} of {len(examples)}')\n",
    "            xml_path = os.path.join(annotations_dir, 'xmls', example + '.xml')\n",
    "            \n",
    "            if not os.path.exists(xml_path):\n",
    "                logging.warning(f'Could not find {xml_path}, ignoring example.')\n",
    "                continue\n",
    "            with tf.gfile.GFile(xml_path, 'r') as fid:\n",
    "                xml_str = fid.read()\n",
    "            xml = etree.fromstring(xml_str)\n",
    "            data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n",
    "\n",
    "            try:\n",
    "                tf_example = dict_to_tf_example(\n",
    "                    data,\n",
    "                    label_map_dict)\n",
    "                if tf_example:\n",
    "                    shard_idx = idx % num_shards\n",
    "                    output_tfrecords[shard_idx].write(\n",
    "                        tf_example.SerializeToString())\n",
    "            except ValueError:\n",
    "                logging.warning(f'Invalid example: {xml_path}, ignoring.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "    if FLAGS.database not in DATASETS:\n",
    "        raise ValueError(f'set must be in : {DATASETS}')\n",
    "    \n",
    "    data_dir = os.getcwd()\n",
    "    label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n",
    "    \n",
    "    logging.info(f'Reading from {FLAGS.database} dataset.')\n",
    "    annotations_dir = os.path.join(data_dir, 'Annotations')\n",
    "    examples_path = os.path.join(annotations_dir, 'trainval.txt')\n",
    "    examples_list = dataset_util.read_examples_list(examples_path)\n",
    "    \n",
    "    # Test images are not included in the downloaded data set, so we shall perform\n",
    "    # our own split.\n",
    "    random.seed(42)\n",
    "    random.shuffle(examples_list)\n",
    "    num_examples = len(examples_list)\n",
    "    num_train = int(0.7 * num_examples)\n",
    "    train_examples = examples_list[:num_train]\n",
    "    val_examples = examples_list[num_train:]\n",
    "    logging.info('%d training and %d validation examples.',\n",
    "        len(train_examples), len(val_examples))\n",
    "    \n",
    "    train_output_path = os.path.join(FLAGS.output_dir, 'train.record')\n",
    "    val_output_path = os.path.join(FLAGS.output_dir, 'val.record')\n",
    "    \n",
    "    create_tf_record(\n",
    "        train_output_path,\n",
    "        FLAGS.num_shards,\n",
    "        label_map_dict,\n",
    "        annotations_dir,\n",
    "        train_examples)\n",
    "    create_tf_record(\n",
    "        val_output_path,\n",
    "        FLAGS.num_shards,\n",
    "        label_map_dict,\n",
    "        annotations_dir,\n",
    "        val_examples)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
